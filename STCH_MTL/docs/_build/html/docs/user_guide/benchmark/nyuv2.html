<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NYUv2 &mdash; LibMTL  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Office-31 and Office-Home" href="office.html" />
    <link rel="prev" title="Run a Benckmark" href="../benchmark.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> LibMTL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/quick_start.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../mtl.html">What is Multi-Task Learning?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../framework.html">Overall Framework</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../benchmark.html">Run a Benckmark</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">NYUv2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#run-a-model">Run a Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="office.html">Office-31 and Office-Home</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../develop/dataset.html">Apply to a New Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../develop/arch.html">Customize an Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../develop/weighting.html">Customize a Weighting Strategy</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/loss/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.loss</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/utils/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.utils</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/model/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.model</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/config/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.config</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/metrics/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.metrics</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/weighting/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.weighting</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_autoapi/LibMTL/architecture/index.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">LibMTL.architecture</span></code></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">LibMTL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../benchmark.html">Run a Benckmark</a> &raquo;</li>
      <li>NYUv2</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/user_guide/benchmark/nyuv2.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="nyuv2">
<h1>NYUv2<a class="headerlink" href="#nyuv2" title="Permalink to this headline">¶</a></h1>
<p>The NYUv2 dataset <span id="id1">[<a class="reference internal" href="#id10" title="Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Proceedings of the 8th European Conference on Computer Vision, 746–760. 2012.">1</a>]</span> is an indoor scene understanding dataset, which consists of video sequences recorded by the RGB and Depth cameras in the Microsoft Kinect. It contains 795 and 654 images with ground-truths for training and validation, respectively.</p>
<p>We used the pre-processed NYUv2 dataset in <span id="id2">[<a class="reference internal" href="#id11" title="Shikun Liu, Edward Johns, and Andrew J. Davison. End-to-end multi-task learning with attention. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 1871–1880. 2019.">2</a>]</span>, which can be downloaded <a class="reference external" href="https://www.dropbox.com/sh/86nssgwm6hm3vkb/AACrnUQ4GxpdrBbLjb6n-mWNa?dl=0">here</a>. Each input image has been resized into 3x288x384 and corresponds to three labels (or tasks): 13-class semantic segmentation, depth estimation, and surface normal prediction. Thus, it is a multi-label dataset, which means <code class="docutils literal notranslate"><span class="pre">multi_input</span></code> must be <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>The training code are mainly modified from <a class="reference external" href="https://github.com/lorenmt/mtan">mtan</a> and available in <code class="docutils literal notranslate"><span class="pre">examples/nyu</span></code>. We used DeepLabV3+ architecture <span id="id3">[<a class="reference internal" href="#id13" title="Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the 14th European Conference on Computer Vision, volume 11211, 833–851. 2018.">3</a>]</span>, where a ResNet-50 network pretrained on the ImageNet dataset with dilated convolutions <span id="id4">[<a class="reference internal" href="#id12" title="Fisher Yu, Vladlen Koltun, and Thomas A. Funkhouser. Dilated residual networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 636–644. 2017.">4</a>]</span> is used as a shared encoder among tasks and the Atrous Spatial Pyramid Pooling (ASPP) <span id="id5">[<a class="reference internal" href="#id13" title="Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the 14th European Conference on Computer Vision, volume 11211, 833–851. 2018.">3</a>]</span> module is used as task-specific head for each task.</p>
<p>Following <span id="id6">[<a class="reference internal" href="#id11" title="Shikun Liu, Edward Johns, and Andrew J. Davison. End-to-end multi-task learning with attention. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, 1871–1880. 2019.">2</a>]</span>, the metric objective of three tasks are as follows. Mean Intersection over Union (mIoU) and Pixel Accuracy (Pix Acc) are used for semantic segmentation task. Absolute and relative errors (Abs Err and Rel Err) are used for depth estimation task. Five metrics are used for surface normal estimation task: mean absolute of the error (Mean), median absolute of the error (Median), and percentage of pixels with angular error below a threshold <span class="math notranslate nohighlight">\({\epsilon}\)</span> with <span class="math notranslate nohighlight">\({\epsilon=11.25^{\circ}, 22.5^{\circ}, 30^{\circ}}\)</span> (abbreviated as &lt;11.25, &lt;22.5, &lt;30). Among them, the higher scores of mIoU, Pix Acc, &lt;11.25, &lt;22.5, and &lt;30 mean the better performance and the lower scores of Abs Err, Rel Err, Mean, and Median indicate the better performance.</p>
<div class="section" id="run-a-model">
<h2>Run a Model<a class="headerlink" href="#run-a-model" title="Permalink to this headline">¶</a></h2>
<p>The script <code class="docutils literal notranslate"><span class="pre">train_nyu.py</span></code> is the main file for training and evaluating a MTL model on the NYUv2 dataset. A set of command-line arguments is provided to allow users to adjust the training parameter configuration.</p>
<p>Some important  arguments are described as follows.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weighting</span></code>: The weighting strategy. Refer to <a class="reference external" href="../_autoapi/LibMTL/weighting/index.html">here</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">arch</span></code>: The MTL architecture. Refer to <a class="reference external" href="../_autoapi/LibMTL/architecture/index.html">here</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_id</span></code>: The id of gpu. Default to ‘0’.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">seed</span></code>: The random seed for reproducibility. Default to 0.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scheduler</span></code>: The type of the learning rate scheduler. We recommend to use ‘step’ here.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optim</span></code>: The type of the optimizer. We recommend to use ‘adam’ here.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dataset_path</span></code>: The path of the NYUv2 dataset.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">aug</span></code>: If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the model is trained with a data augmentation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_bs</span></code>: The batch size of training data. Default to 8.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test_bs</span></code>: The batch size of test data. Default to 8.</p></li>
</ul>
<p>The complete command-line arguments and their descriptions can be found by running the following command.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python train_nyu.py -h
</pre></div>
</div>
<p>If you understand those command-line arguments, you can train a MTL model by running a command like this.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python train_nyu.py --weighting WEIGHTING --arch ARCH --dataset_path PATH/nyuv2 --gpu_id GPU_ID --scheduler step
</pre></div>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="id9"><dl class="citation">
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In <em>Proceedings of the 8th European Conference on Computer Vision</em>, 746–760. 2012.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">2</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Shikun Liu, Edward Johns, and Andrew J. Davison. End-to-end multi-task learning with attention. In <em>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</em>, 1871–1880. 2019.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">3</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id5">2</a>)</span></dt>
<dd><p>Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In <em>Proceedings of the 14th European Conference on Computer Vision</em>, volume 11211, 833–851. 2018.</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Fisher Yu, Vladlen Koltun, and Thomas A. Funkhouser. Dilated residual networks. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 636–644. 2017.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../benchmark.html" class="btn btn-neutral float-left" title="Run a Benckmark" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="office.html" class="btn btn-neutral float-right" title="Office-31 and Office-Home" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Baijiong Lin and Yu Zhang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>